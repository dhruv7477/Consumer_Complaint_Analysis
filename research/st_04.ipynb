{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    training_data: Path\n",
    "    trained_model_path: Path\n",
    "    base_model_path: Path\n",
    "    params_epochs: int\n",
    "    params_learning_rate: float\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareCallbacksConfig:\n",
    "    root_dir: Path\n",
    "    tensorboard_root_log_dir: Path\n",
    "    checkpoint_model_filepath: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Coding_Notes\\Consumer_Complaint_Analysis\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Consumer_Complaint_Analysis.constants import *\n",
    "from Consumer_Complaint_Analysis.components import PrepareBaseModel\n",
    "from Consumer_Complaint_Analysis.utils import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_prepare_callback_config(self) -> PrepareCallbacksConfig:\n",
    "        config = self.config.prepare_callbacks\n",
    "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
    "        create_directories([\n",
    "            Path(model_ckpt_dir),\n",
    "            Path(config.tensorboard_root_log_dir)\n",
    "        ])\n",
    "\n",
    "        prepare_callback_config = PrepareCallbacksConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir),\n",
    "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath)\n",
    "        )\n",
    "\n",
    "        return prepare_callback_config\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model.base_model_path\n",
    "        training_data = self.config.data_ingestion.csv_file_path\n",
    "        create_directories([\n",
    "            Path(training.root_dir)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            base_model_path=Path(prepare_base_model),\n",
    "            training_data=Path(training_data),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            params_epochs=self.params.EPOCHS,\n",
    "            params_learning_rate=self.params.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "        return training_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class PrepareCallback:\n",
    "    def __init__(self, config: PrepareCallbacksConfig):\n",
    "        self.config = config\n",
    "\n",
    "    @property\n",
    "    def _create_tb_writer(self):\n",
    "        timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        tb_running_log_dir = os.path.join(\n",
    "            self.config.tensorboard_root_log_dir,\n",
    "            f\"tb_logs_at_{timestamp}\",\n",
    "        )\n",
    "        return torch.utils.tensorboard.SummaryWriter(tb_running_log_dir)\n",
    "\n",
    "    def get_ckpt_callback(self, model):\n",
    "        def save_ckpt(model, iteration):\n",
    "            torch.nn.util.checkpoint.save(\n",
    "                model,\n",
    "                self.config.checkpoint_model_filepath\n",
    "                )\n",
    "        return save_ckpt\n",
    "\n",
    "        \n",
    "    def get_tb_callback(self, model, inputs, outputs, iteration):\n",
    "        self._create_tb_writer.add_graph(model, inputs)\n",
    "        self._create_tb_writer.add_scalar('Loss/train', outputs, iteration)\n",
    "        self.save_ckpt(model, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "class Training(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "    def load_base_model(self):\n",
    "        model = PrepareBaseModel(config=self.config, df=self.config.training_data)\n",
    "        model.load_state_dict(torch.load(self.config.base_model_path))\n",
    "\n",
    "    def validate(self, validation_dataloader, loss_fn):\n",
    "        model = self.load_base_model() # Load the base model\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "    \n",
    "        # Initialize running validation loss and accuracy\n",
    "        running_val_loss = 0.0\n",
    "        running_val_acc = 0.0\n",
    "        val_samples = 0\n",
    "    \n",
    "        # Turn off gradient calculation to save memory and computation\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_dataloader:\n",
    "                inputs = inputs.to(\"cuda\")\n",
    "                labels = labels.to(\"cuda\")\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                val_samples += 1\n",
    "    \n",
    "                # Compute accuracy by comparing the model's predictions with the actual labels\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_val_acc += (preds == labels).float().mean().item()\n",
    "    \n",
    "        # Calculate average validation loss and accuracy\n",
    "        avg_val_loss = running_val_loss / val_samples\n",
    "        avg_val_acc = running_val_acc / val_samples\n",
    "    \n",
    "        return avg_val_loss, avg_val_acc\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.model = self.load_base_model()\n",
    "        self.criterion = BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{self.config.params_epochs}')\n",
    "            print('-' * 10)\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    self.model.train()\n",
    "                    self.scheduler.step()\n",
    "                else:\n",
    "                    self.model.eval()\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                for inputs, labels in self.dataloaders[phase]:\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = self.model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            self.optimizer.step()\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                epoch_loss = running_loss / len(self.dataloaders[phase].dataset)\n",
    "                epoch_acc = running_corrects.double() / len(self.dataloaders[phase].dataset)\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "        torch.save(self.model.state_dict(), self.config.trained_model_path)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model, path):\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PrepareBaseModel:\n\tUnexpected key(s) in state_dict: \"distilbert.embeddings.word_embeddings.weight\", \"distilbert.embeddings.position_embeddings.weight\", \"distilbert.embeddings.LayerNorm.weight\", \"distilbert.embeddings.LayerNorm.bias\", \"distilbert.transformer.layer.0.attention.q_lin.weight\", \"distilbert.transformer.layer.0.attention.q_lin.bias\", \"distilbert.transformer.layer.0.attention.k_lin.weight\", \"distilbert.transformer.layer.0.attention.k_lin.bias\", \"distilbert.transformer.layer.0.attention.v_lin.weight\", \"distilbert.transformer.layer.0.attention.v_lin.bias\", \"distilbert.transformer.layer.0.attention.out_lin.weight\", \"distilbert.transformer.layer.0.attention.out_lin.bias\", \"distilbert.transformer.layer.0.sa_layer_norm.weight\", \"distilbert.transformer.layer.0.sa_layer_norm.bias\", \"distilbert.transformer.layer.0.ffn.lin1.weight\", \"distilbert.transformer.layer.0.ffn.lin1.bias\", \"distilbert.transformer.layer.0.ffn.lin2.weight\", \"distilbert.transformer.layer.0.ffn.lin2.bias\", \"distilbert.transformer.layer.0.output_layer_norm.weight\", \"distilbert.transformer.layer.0.output_layer_norm.bias\", \"distilbert.transformer.layer.1.attention.q_lin.weight\", \"distilbert.transformer.layer.1.attention.q_lin.bias\", \"distilbert.transformer.layer.1.attention.k_lin.weight\", \"distilbert.transformer.layer.1.attention.k_lin.bias\", \"distilbert.transformer.layer.1.attention.v_lin.weight\", \"distilbert.transformer.layer.1.attention.v_lin.bias\", \"distilbert.transformer.layer.1.attention.out_lin.weight\", \"distilbert.transformer.layer.1.attention.out_lin.bias\", \"distilbert.transformer.layer.1.sa_layer_norm.weight\", \"distilbert.transformer.layer.1.sa_layer_norm.bias\", \"distilbert.transformer.layer.1.ffn.lin1.weight\", \"distilbert.transformer.layer.1.ffn.lin1.bias\", \"distilbert.transformer.layer.1.ffn.lin2.weight\", \"distilbert.transformer.layer.1.ffn.lin2.bias\", \"distilbert.transformer.layer.1.output_layer_norm.weight\", \"distilbert.transformer.layer.1.output_layer_norm.bias\", \"distilbert.transformer.layer.2.attention.q_lin.weight\", \"distilbert.transformer.layer.2.attention.q_lin.bias\", \"distilbert.transformer.layer.2.attention.k_lin.weight\", \"distilbert.transformer.layer.2.attention.k_lin.bias\", \"distilbert.transformer.layer.2.attention.v_lin.weight\", \"distilbert.transformer.layer.2.attention.v_lin.bias\", \"distilbert.transformer.layer.2.attention.out_lin.weight\", \"distilbert.transformer.layer.2.attention.out_lin.bias\", \"distilbert.transformer.layer.2.sa_layer_norm.weight\", \"distilbert.transformer.layer.2.sa_layer_norm.bias\", \"distilbert.transformer.layer.2.ffn.lin1.weight\", \"distilbert.transformer.layer.2.ffn.lin1.bias\", \"distilbert.transformer.layer.2.ffn.lin2.weight\", \"distilbert.transformer.layer.2.ffn.lin2.bias\", \"distilbert.transformer.layer.2.output_layer_norm.weight\", \"distilbert.transformer.layer.2.output_layer_norm.bias\", \"distilbert.transformer.layer.3.attention.q_lin.weight\", \"distilbert.transformer.layer.3.attention.q_lin.bias\", \"distilbert.transformer.layer.3.attention.k_lin.weight\", \"distilbert.transformer.layer.3.attention.k_lin.bias\", \"distilbert.transformer.layer.3.attention.v_lin.weight\", \"distilbert.transformer.layer.3.attention.v_lin.bias\", \"distilbert.transformer.layer.3.attention.out_lin.weight\", \"distilbert.transformer.layer.3.attention.out_lin.bias\", \"distilbert.transformer.layer.3.sa_layer_norm.weight\", \"distilbert.transformer.layer.3.sa_layer_norm.bias\", \"distilbert.transformer.layer.3.ffn.lin1.weight\", \"distilbert.transformer.layer.3.ffn.lin1.bias\", \"distilbert.transformer.layer.3.ffn.lin2.weight\", \"distilbert.transformer.layer.3.ffn.lin2.bias\", \"distilbert.transformer.layer.3.output_layer_norm.weight\", \"distilbert.transformer.layer.3.output_layer_norm.bias\", \"distilbert.transformer.layer.4.attention.q_lin.weight\", \"distilbert.transformer.layer.4.attention.q_lin.bias\", \"distilbert.transformer.layer.4.attention.k_lin.weight\", \"distilbert.transformer.layer.4.attention.k_lin.bias\", \"distilbert.transformer.layer.4.attention.v_lin.weight\", \"distilbert.transformer.layer.4.attention.v_lin.bias\", \"distilbert.transformer.layer.4.attention.out_lin.weight\", \"distilbert.transformer.layer.4.attention.out_lin.bias\", \"distilbert.transformer.layer.4.sa_layer_norm.weight\", \"distilbert.transformer.layer.4.sa_layer_norm.bias\", \"distilbert.transformer.layer.4.ffn.lin1.weight\", \"distilbert.transformer.layer.4.ffn.lin1.bias\", \"distilbert.transformer.layer.4.ffn.lin2.weight\", \"distilbert.transformer.layer.4.ffn.lin2.bias\", \"distilbert.transformer.layer.4.output_layer_norm.weight\", \"distilbert.transformer.layer.4.output_layer_norm.bias\", \"distilbert.transformer.layer.5.attention.q_lin.weight\", \"distilbert.transformer.layer.5.attention.q_lin.bias\", \"distilbert.transformer.layer.5.attention.k_lin.weight\", \"distilbert.transformer.layer.5.attention.k_lin.bias\", \"distilbert.transformer.layer.5.attention.v_lin.weight\", \"distilbert.transformer.layer.5.attention.v_lin.bias\", \"distilbert.transformer.layer.5.attention.out_lin.weight\", \"distilbert.transformer.layer.5.attention.out_lin.bias\", \"distilbert.transformer.layer.5.sa_layer_norm.weight\", \"distilbert.transformer.layer.5.sa_layer_norm.bias\", \"distilbert.transformer.layer.5.ffn.lin1.weight\", \"distilbert.transformer.layer.5.ffn.lin1.bias\", \"distilbert.transformer.layer.5.ffn.lin2.weight\", \"distilbert.transformer.layer.5.ffn.lin2.bias\", \"distilbert.transformer.layer.5.output_layer_norm.weight\", \"distilbert.transformer.layer.5.output_layer_norm.bias\", \"pre_classifier.weight\", \"pre_classifier.bias\", \"classifier.weight\", \"classifier.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     training\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m training_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_training_config()\n\u001b[0;32m      8\u001b[0m training \u001b[39m=\u001b[39m Training(config\u001b[39m=\u001b[39mtraining_config)\n\u001b[1;32m----> 9\u001b[0m training\u001b[39m.\u001b[39;49mload_base_model()\n\u001b[0;32m     10\u001b[0m training\u001b[39m.\u001b[39mvalidate(validation_dataloader\u001b[39m=\u001b[39mtraining\u001b[39m.\u001b[39mload_base_model()\u001b[39m.\u001b[39mdataloaders,loss_fn\u001b[39m=\u001b[39mBCELoss)\n\u001b[0;32m     11\u001b[0m training\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mTraining.load_base_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_base_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     12\u001b[0m     model \u001b[39m=\u001b[39m PrepareBaseModel(config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, df\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtraining_data)\n\u001b[1;32m---> 13\u001b[0m     model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbase_model_path))\n",
      "File \u001b[1;32mf:\\Coding_Notes\\Consumer_Complaint_Analysis\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PrepareBaseModel:\n\tUnexpected key(s) in state_dict: \"distilbert.embeddings.word_embeddings.weight\", \"distilbert.embeddings.position_embeddings.weight\", \"distilbert.embeddings.LayerNorm.weight\", \"distilbert.embeddings.LayerNorm.bias\", \"distilbert.transformer.layer.0.attention.q_lin.weight\", \"distilbert.transformer.layer.0.attention.q_lin.bias\", \"distilbert.transformer.layer.0.attention.k_lin.weight\", \"distilbert.transformer.layer.0.attention.k_lin.bias\", \"distilbert.transformer.layer.0.attention.v_lin.weight\", \"distilbert.transformer.layer.0.attention.v_lin.bias\", \"distilbert.transformer.layer.0.attention.out_lin.weight\", \"distilbert.transformer.layer.0.attention.out_lin.bias\", \"distilbert.transformer.layer.0.sa_layer_norm.weight\", \"distilbert.transformer.layer.0.sa_layer_norm.bias\", \"distilbert.transformer.layer.0.ffn.lin1.weight\", \"distilbert.transformer.layer.0.ffn.lin1.bias\", \"distilbert.transformer.layer.0.ffn.lin2.weight\", \"distilbert.transformer.layer.0.ffn.lin2.bias\", \"distilbert.transformer.layer.0.output_layer_norm.weight\", \"distilbert.transformer.layer.0.output_layer_norm.bias\", \"distilbert.transformer.layer.1.attention.q_lin.weight\", \"distilbert.transformer.layer.1.attention.q_lin.bias\", \"distilbert.transformer.layer.1.attention.k_lin.weight\", \"distilbert.transformer.layer.1.attention.k_lin.bias\", \"distilbert.transformer.layer.1.attention.v_lin.weight\", \"distilbert.transformer.layer.1.attention.v_lin.bias\", \"distilbert.transformer.layer.1.attention.out_lin.weight\", \"distilbert.transformer.layer.1.attention.out_lin.bias\", \"distilbert.transformer.layer.1.sa_layer_norm.weight\", \"distilbert.transformer.layer.1.sa_layer_norm.bias\", \"distilbert.transformer.layer.1.ffn.lin1.weight\", \"distilbert.transformer.layer.1.ffn.lin1.bias\", \"distilbert.transformer.layer.1.ffn.lin2.weight\", \"distilbert.transformer.layer.1.ffn.lin2.bias\", \"distilbert.transformer.layer.1.output_layer_norm.weight\", \"distilbert.transformer.layer.1.output_layer_norm.bias\", \"distilbert.transformer.layer.2.attention.q_lin.weight\", \"distilbert.transformer.layer.2.attention.q_lin.bias\", \"distilbert.transformer.layer.2.attention.k_lin.weight\", \"distilbert.transformer.layer.2.attention.k_lin.bias\", \"distilbert.transformer.layer.2.attention.v_lin.weight\", \"distilbert.transformer.layer.2.attention.v_lin.bias\", \"distilbert.transformer.layer.2.attention.out_lin.weight\", \"distilbert.transformer.layer.2.attention.out_lin.bias\", \"distilbert.transformer.layer.2.sa_layer_norm.weight\", \"distilbert.transformer.layer.2.sa_layer_norm.bias\", \"distilbert.transformer.layer.2.ffn.lin1.weight\", \"distilbert.transformer.layer.2.ffn.lin1.bias\", \"distilbert.transformer.layer.2.ffn.lin2.weight\", \"distilbert.transformer.layer.2.ffn.lin2.bias\", \"distilbert.transformer.layer.2.output_layer_norm.weight\", \"distilbert.transformer.layer.2.output_layer_norm.bias\", \"distilbert.transformer.layer.3.attention.q_lin.weight\", \"distilbert.transformer.layer.3.attention.q_lin.bias\", \"distilbert.transformer.layer.3.attention.k_lin.weight\", \"distilbert.transformer.layer.3.attention.k_lin.bias\", \"distilbert.transformer.layer.3.attention.v_lin.weight\", \"distilbert.transformer.layer.3.attention.v_lin.bias\", \"distilbert.transformer.layer.3.attention.out_lin.weight\", \"distilbert.transformer.layer.3.attention.out_lin.bias\", \"distilbert.transformer.layer.3.sa_layer_norm.weight\", \"distilbert.transformer.layer.3.sa_layer_norm.bias\", \"distilbert.transformer.layer.3.ffn.lin1.weight\", \"distilbert.transformer.layer.3.ffn.lin1.bias\", \"distilbert.transformer.layer.3.ffn.lin2.weight\", \"distilbert.transformer.layer.3.ffn.lin2.bias\", \"distilbert.transformer.layer.3.output_layer_norm.weight\", \"distilbert.transformer.layer.3.output_layer_norm.bias\", \"distilbert.transformer.layer.4.attention.q_lin.weight\", \"distilbert.transformer.layer.4.attention.q_lin.bias\", \"distilbert.transformer.layer.4.attention.k_lin.weight\", \"distilbert.transformer.layer.4.attention.k_lin.bias\", \"distilbert.transformer.layer.4.attention.v_lin.weight\", \"distilbert.transformer.layer.4.attention.v_lin.bias\", \"distilbert.transformer.layer.4.attention.out_lin.weight\", \"distilbert.transformer.layer.4.attention.out_lin.bias\", \"distilbert.transformer.layer.4.sa_layer_norm.weight\", \"distilbert.transformer.layer.4.sa_layer_norm.bias\", \"distilbert.transformer.layer.4.ffn.lin1.weight\", \"distilbert.transformer.layer.4.ffn.lin1.bias\", \"distilbert.transformer.layer.4.ffn.lin2.weight\", \"distilbert.transformer.layer.4.ffn.lin2.bias\", \"distilbert.transformer.layer.4.output_layer_norm.weight\", \"distilbert.transformer.layer.4.output_layer_norm.bias\", \"distilbert.transformer.layer.5.attention.q_lin.weight\", \"distilbert.transformer.layer.5.attention.q_lin.bias\", \"distilbert.transformer.layer.5.attention.k_lin.weight\", \"distilbert.transformer.layer.5.attention.k_lin.bias\", \"distilbert.transformer.layer.5.attention.v_lin.weight\", \"distilbert.transformer.layer.5.attention.v_lin.bias\", \"distilbert.transformer.layer.5.attention.out_lin.weight\", \"distilbert.transformer.layer.5.attention.out_lin.bias\", \"distilbert.transformer.layer.5.sa_layer_norm.weight\", \"distilbert.transformer.layer.5.sa_layer_norm.bias\", \"distilbert.transformer.layer.5.ffn.lin1.weight\", \"distilbert.transformer.layer.5.ffn.lin1.bias\", \"distilbert.transformer.layer.5.ffn.lin2.weight\", \"distilbert.transformer.layer.5.ffn.lin2.bias\", \"distilbert.transformer.layer.5.output_layer_norm.weight\", \"distilbert.transformer.layer.5.output_layer_norm.bias\", \"pre_classifier.weight\", \"pre_classifier.bias\", \"classifier.weight\", \"classifier.bias\". "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_callbacks_config = config.get_prepare_callback_config()\n",
    "    prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
    "    callback_list = [prepare_callbacks.get_ckpt_callback, prepare_callbacks.get_tb_callback]\n",
    "    \n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.load_base_model()\n",
    "    training.validate(validation_dataloader=training.load_base_model().dataloaders,loss_fn=BCELoss)\n",
    "    training.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0933eedb68cbe398ff033e891e3deb75331952e6373c8260509fd558998e38d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
